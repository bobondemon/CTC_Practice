{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DvgJT9G6dUrM"
   },
   "source": [
    "# CTC Forward, Backward and Gradient Practice. Compared with tf.ctc_loss.\n",
    "\n",
    "Credit 是此篇 [DingKe ipynb](https://github.com/DingKe/ml-tutorial/blob/master/ctc/CTC.ipynb) 的, 他完整呈現了 CTC loss 以及 gradient 的計算, 非常棒!\n",
    "\n",
    "此筆記加入自己的說明, 並且最後使用 tensorflow 來驗證.\n",
    "\n",
    "這篇另一個主要目的為改成可以練習的格式 (**#TODO tag**). 因為我相信最好的學習方式是自己造一次輪子, 所以可以的話, 請試著完成把 #TODO tag 的部分做完吧.\n",
    "\n",
    "我們只專注在 CTC loss 的 forward, backwark and gradient. Decoding 部分請參考原作者的 [ipynb](https://github.com/DingKe/ml-tutorial/blob/master/ctc/CTC.ipynb). 最後使用 tf.nn.ctc_loss and tf.gradients 與我們的計算做對比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "J45YJwIgXnJ5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lHrFPI-FenD9"
   },
   "source": [
    "## 一些變數定義\n",
    "\n",
    "1. `Vocab = [0, 1, 2, 3, 4]`, 其中 **0 表示 'blank'**.\n",
    "\n",
    "2. `V = len(Vocab) = 5` 是字典大小, 譬如字典有 4 個 labels 加上一個 blank, 因此 V=5\n",
    "\n",
    "3. `l` 是正確答案 (已經安插 blanks 了), 例如 `l = [0, 3, 0, 3, 0, 4, 0]`\n",
    "\n",
    "4. `L = len(l) = 7`\n",
    "\n",
    "5. **後驗概率 `y`** (shape=`[V,T]`), 其中 `T` 表示 input sequence 長度, 所以 `y[k,t]` 表示時間點 `t`, label `k` 的後驗概率\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ZoDrr_F1d7mp"
   },
   "outputs": [],
   "source": [
    "Vocab = [0,1,2,3,4]\n",
    "l = [0, 3, 0, 3, 0, 4, 0]\n",
    "V, L = len(Vocab), len(l)\n",
    "T = 12\n",
    "logits = np.random.random([V,T])\n",
    "\n",
    "def softmax(logits):\n",
    "    max_value = np.max(logits, axis=0, keepdims=True)\n",
    "    exp = np.exp(logits - max_value)\n",
    "    exp_sum = np.sum(exp, axis=0, keepdims=True)\n",
    "    dist = exp / exp_sum\n",
    "    return dist\n",
    "  \n",
    "y = softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YHAZYIwgpZc3"
   },
   "source": [
    "## Forward/Backward Dynamic Programming\n",
    "\n",
    "整個 CTC 關鍵就在計算 Dynamic Programming Table, 我們需要計算如下圖的 DP Table:\n",
    "<img src=\"https://i.imgur.com/lOPaABD.png\" height=\"70%\" width=\"70%\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "DVIvRsnXiv6l"
   },
   "outputs": [],
   "source": [
    "def forward(y, label):\n",
    "  L = len(label)\n",
    "  V, T = y.shape\n",
    "  alpha = np.zeros([L,T])\n",
    "  # init first column\n",
    "  alpha[0,0] = # TODO\n",
    "  alpha[1,0] = # TODO\n",
    "  # run dp\n",
    "  for t in range(1,T):\n",
    "    for s in range(L):\n",
    "      k = label[s]\n",
    "      y_k_t = y[k,t]\n",
    "      alpha_tmp = alpha[s,t-1]\n",
    "      if s>0:\n",
    "        alpha_tmp += # TODO\n",
    "      if s>1 and k!=0 and k!=label[s-2]:\n",
    "        alpha_tmp += # TODO\n",
    "      alpha[s,t] = alpha_tmp*y_k_t\n",
    "  return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "y78iljgTAWlg"
   },
   "outputs": [],
   "source": [
    "def backward(y,label):\n",
    "  L = len(label)\n",
    "  V, T = y.shape\n",
    "  beta = np.zeros([L,T])\n",
    "  # init last column\n",
    "  beta[-1,-1] = # TODO\n",
    "  beta[-2,-1] = # TODO\n",
    "  # run dp\n",
    "  for t in range(T-2,-1,-1):\n",
    "    for s in range(L):\n",
    "      k = label[s]\n",
    "      y_k_t = y[k,t]\n",
    "      beta_tmp = beta[s,t+1]\n",
    "      if s<L-1:\n",
    "        beta_tmp += # TODO\n",
    "      if s<L-2 and k!=0 and k!=label[s+2]:\n",
    "        beta_tmp += # TODO\n",
    "      beta[s,t] = beta_tmp*y_k_t\n",
    "  return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kWg7qgg5EPbt"
   },
   "outputs": [],
   "source": [
    "# Forward and Backward likelihood should be very close\n",
    "\n",
    "alpha = forward(y,l)\n",
    "likelihood_by_forward =  alpha[-1,-1] + alpha[-2,-1]\n",
    "print('likelihood_by_forward = {}'.format(likelihood_by_forward))\n",
    "\n",
    "beta = backward(y,l)\n",
    "likelihood_by_backword =  beta[0,0] + beta[1,0]\n",
    "print('likelihood_by_backword = {}'.format(likelihood_by_backword))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3nwPd5WCFf5A"
   },
   "source": [
    "## Gradient w.r.t. Posterior $y$\n",
    "\n",
    "[論文](https://www.cs.toronto.edu/~graves/icml_2006.pdf)的 (14) 如下:\n",
    "\n",
    "$$\n",
    "p(l|x)=\\sum_{s=1}^{|l'|}\\frac{\\alpha_t(s)\\beta_t(s)}{y_{l_s}^t}\n",
    "$$\n",
    "\n",
    "注意微分時 $\\alpha_t(s)$ 與 $\\beta_t(s)$ 是跟 $y_{l_s}^t$ 有關, 不能視為 constant, 利用它們的定義直接展開求微分即可. 譬如我們以 $t=2$ 當例子, 並且假設只有一條 alignment $\\pi=\\{\\pi_0,\\pi_1,...,\\pi_T\\}$, 其中 $y_{\\pi_2}^2=y_{l'_s}^2$, 就是先忽略 $\\sum_{\\pi\\in B^{-1}(l)}$, 不影響推導\n",
    "\n",
    "$$\n",
    "p(l|x)=\\sum_{s=1}^{|l'|}\\frac{\\alpha_2(s)\\beta_2(s)}{y_{l'_s}^2}\\\\\n",
    "=\\sum_{s=1}^{|l'|}\\frac{y_{\\pi_0}^0 y_{\\pi_1}^1 (y_{l'_s}^2)^2 y_{\\pi_3}^3 ... y_{\\pi_T}^T}{y_{l'_s}^2}\\\\\n",
    "=\\sum_{s=1}^{|l'|} {y_{\\pi_0}^0 y_{\\pi_1}^1 (y_{l'_s}^2) y_{\\pi_3}^3 ... y_{\\pi_T}^T}\\\\\n",
    "\\frac{\\partial p(l|x)}{\\partial y_{l'_s}^2}={y_{\\pi_0}^0 y_{\\pi_1}^1 y_{\\pi_3}^3 ... y_{\\pi_T}^T} \\mbox{,  只保留 index }s\\\\\n",
    "=\\frac{\\alpha_2(s)\\beta_2(s)}{(y_{l'_s}^2)^2}\n",
    "$$\n",
    "\n",
    "因此得到 Gradient:\n",
    "$$\n",
    "\\frac{\\partial p(l|x)}{\\partial y_k^t}=\\frac{1}{{y_k^t}^2}\\sum_{s\\in lab(l,k)}\\alpha_t(s)\\beta_t(s)\n",
    "$$\n",
    "\n",
    "解釋一下 $lab(l,k)$, 以我們上面的例子來說, `l = [0, 3, 0, 3, 0, 4, 0]`, `Vocabulary = [0,1,2,3,4]`, 而 $k$ 是指 Vocabulary 的哪一個值\n",
    "\n",
    "- $lab(l,0)=\\{0,2,4,6\\}$\n",
    "- $lab(l,1)=\\{\\}$\n",
    "- $lab(l,2)=\\{\\}$\n",
    "- $lab(l,3)=\\{1,3\\}$\n",
    "- $lab(l,4)=\\{5\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5syn0MgnblM7"
   },
   "outputs": [],
   "source": [
    "def lab_l_k(l,k):\n",
    "  return ## TODO\n",
    "print(lab_l_k(l,0))  # should be = [0, 2, 4, 6]\n",
    "print(lab_l_k(l,1))  # should be = []\n",
    "print(lab_l_k(l,3))  # should be = [1, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fRrwGv8AdnG_"
   },
   "source": [
    "因此我們的 gradient 計算公式如下\n",
    "$$\n",
    "\\frac{\\partial \\ln p(l|x)}{\\partial y_k^t}=\\frac{1}{p(l|x)}\\frac{\\partial p(l|x)}{\\partial y_k^t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "h8rxUftKFdZS"
   },
   "outputs": [],
   "source": [
    "def gradient(y, label):\n",
    "  L = len(label)\n",
    "  V, T = y.shape\n",
    "  alpha = forward(y,label)\n",
    "  beta = backward(y,label)\n",
    "  grad = np.zeros([V,T])\n",
    "  p = alpha[-1,-1] + alpha[-2,-1]\n",
    "  for t in range(T):\n",
    "    for k in range(V):\n",
    "      for s in lab_l_k(label,k):\n",
    "        grad[k,t] += # TODO\n",
    "  grad /= (y*y*p)\n",
    "  return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "DinVl4zLeGMh"
   },
   "outputs": [],
   "source": [
    "grad = gradient(y, l)\n",
    "print(grad)\n",
    "print(grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Aie-FDvceV4E"
   },
   "source": [
    "用數值方法確認 gradient 計算, $\\frac{df(x)}{dx}\\simeq \\frac{f(x+\\delta)-f(x-\\delta)}{2\\delta}$, 如果我們只用 $\\frac{df(x)}{dx}\\simeq \\frac{f(x+\\delta)-f(x)}{\\delta}$ 可以嗎? 是可以, 但是精確度較差, 原因可以由泰勒展開式知道:\n",
    "$$\n",
    "f(x+\\delta)=f(x)+f'(x)\\delta+\\frac{1}{2}f''(x)\\delta^2+o(|\\delta|^3)\\\\\n",
    "f(x-\\delta)=f(x)-f'(x)\\delta+\\frac{1}{2}f''(x)\\delta^2+o(|\\delta|^3)\\\\\n",
    "$$\n",
    "\n",
    "比較兩種方式的計算結果:\n",
    "\n",
    "$$\n",
    "\\frac{f(x+\\delta)-f(x-\\delta)}{2\\delta}=f'(x)+o(|\\delta|^2)\\\\\n",
    "\\frac{f(x+\\delta)-f(x)}{\\delta}=f'(x)+\\frac{1}{2}f''(x)\\delta+o(|\\delta|^2)=f'(x)+o(|\\delta|)\n",
    "$$\n",
    "\n",
    "可以發現 $\\frac{df(x)}{dx}\\simeq \\frac{f(x+\\delta)-f(x)}{\\delta}$ 估計的 error 高出一個 order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "rgdDwpBeeVQE"
   },
   "outputs": [],
   "source": [
    "def check_grad(y, label, k=-1, t=-1, toleration=1e-3):\n",
    "  grad_1 = gradient(y, label)[k, t]\n",
    "\n",
    "  delta = 1e-10\n",
    "  original = y[k, t]\n",
    "\n",
    "  y[k, t] = original + delta\n",
    "  alpha = forward(y, label)\n",
    "  log_p1 = np.log(alpha[-1,-1] + alpha[-2,-1])\n",
    "\n",
    "  y[k, t] = original - delta\n",
    "  alpha = forward(y, label)\n",
    "  log_p2 = np.log(alpha[-1,-1] + alpha[-2,-1])\n",
    "\n",
    "  y[k, t] = original\n",
    "\n",
    "  grad_2 = # TODO\n",
    "  if np.abs(grad_1 - grad_2) > toleration:\n",
    "    print('[%d, %d]：%.2e' % (k, t, np.abs(grad_1 - grad_2)))\n",
    "\n",
    "for toleration in [1e-5, 1e-6]:\n",
    "  print('%.e' % toleration)\n",
    "  for w in range(y.shape[0]):\n",
    "    for v in range(y.shape[1]):\n",
    "      check_grad(y, l, w, v, toleration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uNSuvN4ngghn"
   },
   "source": [
    "## Gradient w.r.t. Logits $u$\n",
    "\n",
    "<img src=\"https://i.imgur.com/1k0g7N2.png\" height=\"40%\" width=\"40%\">\n",
    "\n",
    "如上圖, 我們已經計算了 $\\frac{\\partial L}{\\partial y_i}$ (這邊假定固定的 $t$, 因此省略不寫), 我們希望計算 $\\frac{\\partial L}{\\partial u_k}$, 由 chain rule 知道如下關係\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial u_k}=\\sum_i^V\\frac{\\partial L}{\\partial y_i}\\frac{\\partial y_i}{\\partial u_k}\n",
    "$$\n",
    "\n",
    "而 softmax 的 gradient 如下:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial u_k}=y_i(\\delta_{ik}-y_k)\n",
    "$$\n",
    "\n",
    "所以結合起來變成:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial u_k}=\\sum_i^V\\frac{\\partial L}{\\partial y_i}\\frac{\\partial y_i}{\\partial u_k}\\\\\n",
    "=\\sum_i^V \\left(\\frac{1}{p(l|x){y_i}^2}\\sum\\alpha\\beta\\right)y_i(\\delta_{ik}-y_k)\\\\\n",
    "=\\sum_i^V \\left(\\frac{\\sum\\alpha\\beta}{p(l|x)y_i}\\right)(\\delta_{ik}-y_k)\\\\\n",
    "=\\sum_i^V \\left(\\frac{\\sum\\alpha\\beta}{p(l|x)y_i}\\right)\\delta_{ik}-y_k\\sum_i^V \\left(\\frac{\\sum\\alpha\\beta}{p(l|x)y_i}\\right)\\\\\n",
    "=\\frac{\\sum\\alpha\\beta}{p(l|x)y_k}-y_k\n",
    "$$ \n",
    "\n",
    "最後一行的第一個 term 是因為只有 $i=k$時 $\\delta_{ik}=1$, 第二個 term 是因為 $\\sum_i^V\\frac{\\sum\\alpha\\beta}{y_i}=p(l|x)$, 請參考論文 $\\frac{\\alpha\\beta}{y_i}$的物理意義\n",
    "\n",
    "總結來說, 將原來 gradient 計算:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial y_i}=\\frac{\\sum\\alpha\\beta}{p(l|x){y_i}^2}\n",
    "$$\n",
    "改成如下計算:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial u_k}=\\frac{\\sum\\alpha\\beta}{p(l|x)y_k}-y_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "fZNt3xUJgXXo"
   },
   "outputs": [],
   "source": [
    "def gradient_logits_naive(y, label):\n",
    "  '''\n",
    "  gradient by back propagation\n",
    "  '''\n",
    "  y_grad = gradient(y, label)\n",
    "\n",
    "  sum_y_grad = np.sum(y_grad * y, axis=0, keepdims=True)\n",
    "  u_grad = y * (y_grad - sum_y_grad) \n",
    "\n",
    "  return u_grad\n",
    "\n",
    "def gradient_logits(y, label):\n",
    "  L = len(label)\n",
    "  V, T = y.shape\n",
    "  alpha = forward(y,label)\n",
    "  beta = backward(y,label)\n",
    "  grad = np.zeros([V,T])\n",
    "  p = alpha[-1,-1] + alpha[-2,-1]\n",
    "  for t in range(T):\n",
    "    for k in range(V):\n",
    "      for s in lab_l_k(label,k):\n",
    "        grad[k,t] += # TODO\n",
    "  grad /= (y*p)\n",
    "  grad -= # TODO\n",
    "  return grad\n",
    "\n",
    "grad_l = gradient_logits_naive(logits, l)\n",
    "grad_2 = gradient_logits(logits, l)\n",
    "\n",
    "print(np.sum(np.abs(grad_l - grad_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k7mqs3_3iiZn"
   },
   "source": [
    "## Tensorflow Gradient w.r.t. Logits $u$\n",
    "\n",
    "使用 tensorflow [`tf.nn.ctc_loss`](https://www.tensorflow.org/api_docs/python/tf/nn/ctc_loss) 來驗證我們的計算\n",
    "\n",
    "tf.nn.ctc_loss(\n",
    "    **labels,\n",
    "    inputs,\n",
    "    sequence_length**,\n",
    "    preprocess_collapse_repeated=False,\n",
    "    ctc_merge_repeated=True,\n",
    "    ignore_longer_outputs_than_inputs=False,\n",
    "    time_major=True\n",
    ")\n",
    "\n",
    "很多參數, 我們就看主要的前三個\n",
    "- `labels`: An int32 SparseTensor. labels.indices[i, :] == [b, t] means labels.values[i] stores the id for (batch b, time t). labels.values[i] must take on values in [0, **num_labels**). See core/ops/ctc_ops.cc for more details.\n",
    "- `inputs`: 3-D float Tensor. If time_major == False, this will be a Tensor shaped: [batch_size, max_time, **num_classes**]. If time_major == True (default), this will be a Tensor shaped: [max_time, batch_size, **num_classes**]. The logits.\n",
    "- `sequence_length`: 1-D int32 vector, size [batch_size]. The sequence lengths.\n",
    "\n",
    "\n",
    "其中有四點要注意:\n",
    "1. `labels` 用的是 num_labels, 而 `inputs` 用的是 num_classes, 它們的關係為: num_classes = num_labels + 1. 所以 num_labels 表示的是 true labels, 而 num_classes 是多了一個 'blank' 的數量\n",
    "2. 'blank' 的 index 不像我們上面一樣用 0 表示, 會是 num_classes-1.\n",
    "3. tf 的 ctc_loss 用的 input 直接就是 logits $u$, 不像我們上面算 gradient 時給的是後驗概率 $y$\n",
    "4. tf 是 loss, 不是 likelihood, 所以算的 gradient 跟我們上面的正負號相反\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zmjQfYfF4KN3"
   },
   "source": [
    "#### 1. 建立 inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "KSFarG8a4J6Z"
   },
   "outputs": [],
   "source": [
    "# logits.shape = y.shape = [V,T]\n",
    "perm_idx = list(range(1,V))+[0]\n",
    "inputs = logits[perm_idx,:].T.reshape([T,1,V]) # [max_time, batch_size, num_classes]\n",
    "inputs_tensor = tf.constant(inputs, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bwEF7QTY4fTo"
   },
   "source": [
    "#### 2. 建立 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "-ku-Nf3S4lf-"
   },
   "outputs": [],
   "source": [
    "# 原來 l = [0, 3, 0, 3, 0, 4, 0], 在這邊要變成 [4, 2, 4, 2, 4, 3, 4], true labels = {0,1,2,3}, blank={4}\n",
    "# blank 不用自己填, 因此給 tensorflow 只需要 [2,2,3] 這樣的 array\n",
    "labels_tensor = tf.SparseTensor(indices=[[0, 0], [0, 1], [0, 2]], values=[2, 2, 3], dense_shape=[1, 3])# 原來 l = [0, 3, 0, 3, 0, 4, 0], 在這邊要變成 [4, 2, 4, 2, 4, 3, 4], true labels = {0,1,2,3}, blank={4} (blank 不用自己填)\n",
    "labels_tensor = tf.SparseTensor(indices=[[0, 1], [0, 3], [0, 5]], values=[2, 2, 3], dense_shape=[1, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i_iccAlu5dt2"
   },
   "source": [
    "### 3. 建立 ctc_loss, 和計算 gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "HKoPZasV5gmt"
   },
   "outputs": [],
   "source": [
    "ctc_loss = tf.nn.ctc_loss(labels_tensor, inputs_tensor, [12])\n",
    "print('inputs_tensor={}\\nctc_loss={}'.format(inputs_tensor,ctc_loss))\n",
    "tf_grad = tf.gradients(ys=ctc_loss,xs=inputs_tensor)\n",
    "with tf.Session() as sess:\n",
    "  tf_grad_results = sess.run(tf_grad)\n",
    "print(np.array(tf_grad_results).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hcw1Vd095tmx"
   },
   "source": [
    "### 4. 比較 tf 算的 gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "_OHiPJLP5z4_"
   },
   "outputs": [],
   "source": [
    "# tf 是 loss, 我們之前算的是 likelihood, 因此差一個正負號\n",
    "tf_grad_results = -np.array(tf_grad_results).reshape([T,V]).T\n",
    "\n",
    "# tf 的 'blank' 是最大的 index, 將他挪回 index 0, true index 全部加 1\n",
    "perm_idx = [V-1]+list(range(0,V-1))\n",
    "tf_grad_results = tf_grad_results[perm_idx,:]\n",
    "\n",
    "# print(tf_grad_results)\n",
    "# print(np.array(tf_grad_results).shape)\n",
    "\n",
    "# 我們之前計算的 gradient\n",
    "grad = gradient_logits(y, l)\n",
    "# print(grad)\n",
    "# print(grad.shape)\n",
    "\n",
    "# compare\n",
    "diff_with_tf = np.mean(np.abs(tf_grad_results - grad))\n",
    "print(diff_with_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "guASq8jE08Xd"
   },
   "source": [
    "## Log-scale Forward/Backward/Gradient\n",
    "\n",
    " 採用 Log scale 的前向後向以及 Gradient 計算會穩定很多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "djTvHNhTrcMV"
   },
   "outputs": [],
   "source": [
    "ninf = -np.float('inf')\n",
    "\n",
    "def _logsumexp(a, b):\n",
    "    '''\n",
    "    np.log(np.exp(a) + np.exp(b))\n",
    "\n",
    "    '''\n",
    "    \n",
    "    if a < b:\n",
    "        a, b = b, a\n",
    "        \n",
    "    if b == ninf:\n",
    "        return a\n",
    "    else:\n",
    "        return a + np.log(1 + np.exp(b - a)) \n",
    "    \n",
    "def logsumexp(*args):\n",
    "    '''\n",
    "    from scipy.special import logsumexp\n",
    "    logsumexp(args)\n",
    "    '''\n",
    "    res = args[0]\n",
    "    for e in args[1:]:\n",
    "        res = _logsumexp(res, e)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "hT5PT9DG1HgB"
   },
   "outputs": [],
   "source": [
    "def forward_log(log_y, label):\n",
    "  L = len(label)\n",
    "  V, T = log_y.shape\n",
    "  alpha = np.ones([L,T])*ninf\n",
    "  # init first column\n",
    "  alpha[0,0] = log_y[label[0],0]\n",
    "  alpha[1,0] = log_y[label[1],0]\n",
    "  # run dp\n",
    "  for t in range(1,T):\n",
    "    for s in range(L):\n",
    "      ls = label[s]\n",
    "      log_y_ls_t = log_y[ls,t]\n",
    "      alpha_tmp = alpha[s,t-1]\n",
    "      if s>0:\n",
    "        alpha_tmp = # TODO\n",
    "      if s>1 and ls!=0 and ls!=label[s-2]:\n",
    "        alpha_tmp = # TODO\n",
    "      alpha[s,t] = alpha_tmp + log_y_ls_t\n",
    "  return alpha\n",
    "\n",
    "log_alpha = forward_log(np.log(y), l)\n",
    "alpha = forward(y, l)\n",
    "print(np.sum(np.abs(np.exp(log_alpha) - alpha)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Ivd945tX2GWE"
   },
   "outputs": [],
   "source": [
    "def backward_log(log_y, label):\n",
    "  L = len(label)\n",
    "  V, T = log_y.shape\n",
    "  beta = np.ones([L,T])*ninf\n",
    "  # init last column\n",
    "  beta[-1,-1] = log_y[label[-1],-1]\n",
    "  beta[-2,-1] = log_y[label[-2],-1]\n",
    "  # run dp\n",
    "  for t in range(T-2,-1,-1):\n",
    "    for s in range(L):\n",
    "      ls = label[s]\n",
    "      log_y_ls_t = log_y[ls,t]\n",
    "      beta_tmp = beta[s,t+1]\n",
    "      if s<L-1:\n",
    "        beta_tmp = logsumexp(beta_tmp,beta[s+1,t+1])\n",
    "      if s<L-2 and ls!=0 and ls!=label[s+2]:\n",
    "        beta_tmp = logsumexp(beta_tmp,beta[s+2,t+1])\n",
    "      beta[s,t] = beta_tmp + log_y_ls_t\n",
    "  return beta\n",
    "\n",
    "log_beta = backward_log(np.log(y), l)\n",
    "beta = backward(y, l)\n",
    "print(np.sum(np.abs(np.exp(log_beta) - beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Ou_iq-Vr2oAM"
   },
   "outputs": [],
   "source": [
    "def gradient_log(log_y, label):\n",
    "  L = len(label)\n",
    "  V, T = log_y.shape\n",
    "  log_alpha = forward_log(log_y,label)\n",
    "  log_beta = backward_log(log_y,label)\n",
    "  log_grad = np.ones([V,T])*ninf\n",
    "  log_p = logsumexp(log_alpha[-1,-1], log_alpha[-2,-1])\n",
    "  for t in range(T):\n",
    "    for k in range(V):\n",
    "      for s in lab_l_k(label,k):\n",
    "        log_grad[k,t] = # TODO\n",
    "  log_grad -= (2*log_y+log_p)\n",
    "  return log_grad\n",
    "\n",
    "log_grad = gradient_log(np.log(y), l)\n",
    "grad = gradient(y, l)\n",
    "print(np.sum(np.abs(np.exp(log_grad) - grad)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6oOggns39hxO"
   },
   "source": [
    "Scaling 方法跳過"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "thdZuTmKu9jZ"
   },
   "source": [
    "## Tensorflow ctc_decoder\n",
    "\n",
    "使用上沒啥好說的, 有一點要注意的是如同上面說的 tf 會將最大的 label 視為 'blank', 所以下面 decode 會看到 label 0 但是沒有 label 4, 另外, 預設 merge_repeated=True 會去掉 blank 和 重複"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pXDDZaYMypU_"
   },
   "source": [
    "tf.nn.ctc_greedy_decoder(\n",
    "    inputs,\n",
    "    sequence_length,\n",
    "    merge_repeated=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "6mnBUFH5vQXk"
   },
   "outputs": [],
   "source": [
    "decoded, neg_sum_logits = tf.nn.ctc_greedy_decoder(inputs_tensor, [12])\n",
    "with tf.Session() as sess:\n",
    "  decoded_out, neg_sum_logits_out = sess.run([decoded, neg_sum_logits])\n",
    "print(decoded_out)\n",
    "print(neg_sum_logits_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oWB5DXp_yRm4"
   },
   "source": [
    "tf.nn.ctc_beam_search_decoder(\n",
    "    inputs,\n",
    "    sequence_length,\n",
    "    beam_width=100,\n",
    "    top_paths=1,\n",
    "    merge_repeated=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "8yWBVuD3yWZc"
   },
   "outputs": [],
   "source": [
    "decoded_beam, neg_sum_logits_beam = tf.nn.ctc_greedy_decoder(inputs_tensor, [12])\n",
    "with tf.Session() as sess:\n",
    "  decoded_beam_out, neg_sum_logits_beam_out = sess.run([decoded_beam, neg_sum_logits_beam])\n",
    "print(decoded_beam_out)\n",
    "print(neg_sum_logits_beam_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RB0ryUsVzirA"
   },
   "source": [
    "更多 decoding 的算法細節, 請參考 [DingKe](https://github.com/DingKe/ml-tutorial/blob/master/ctc/CTC.ipynb) 的文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "7aqnGyac4cxq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CTC_Practice.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
