{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CTC_Practice_Answer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "DvgJT9G6dUrM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CTC Forward, Backward and Gradient Practice. Compared with tf.ctc_loss.\n",
        "\n",
        "Credit 是此篇 [DingKe ipynb](https://github.com/DingKe/ml-tutorial/blob/master/ctc/CTC.ipynb) 的, 他完整呈現了 CTC loss 以及 gradient 的計算, 非常棒!\n",
        "\n",
        "此筆記加入自己的說明, 並且最後使用 tensorflow 來驗證.\n",
        "\n",
        "這篇另一個主要目的為改成可以練習的格式 (**#TODO tag**). 因為我相信最好的學習方式是自己造一次輪子. (這篇是參考答案)\n",
        "\n",
        "我們只專注在 CTC loss 的 forward, backwark and gradient. Decoding 部分請參考原作者的 [ipynb](https://github.com/DingKe/ml-tutorial/blob/master/ctc/CTC.ipynb). 最後使用 tf.nn.ctc_loss and tf.gradients 與我們的計算做對比"
      ]
    },
    {
      "metadata": {
        "id": "J45YJwIgXnJ5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lHrFPI-FenD9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 一些變數定義\n",
        "\n",
        "1. `Vocab = [0, 1, 2, 3, 4]`, 其中 **0 表示 'blank'**.\n",
        "\n",
        "2. `V = len(Vocab) = 5` 是字典大小, 譬如字典有 4 個 labels 加上一個 blank, 因此 V=5\n",
        "\n",
        "3. `l` 是正確答案 (已經安插 blanks 了), 例如 `l = [0, 3, 0, 3, 0, 4, 0]`\n",
        "\n",
        "4. `L = len(l) = 7`\n",
        "\n",
        "5. **後驗概率 `y`** (shape=`[V,T]`), 其中 `T` 表示 input sequence 長度, 所以 `y[k,t]` 表示時間點 `t`, label `k` 的後驗概率\n"
      ]
    },
    {
      "metadata": {
        "id": "ZoDrr_F1d7mp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Vocab = [0,1,2,3,4]\n",
        "l = [0, 3, 0, 3, 0, 4, 0]\n",
        "V, L = len(Vocab), len(l)\n",
        "T = 12\n",
        "logits = np.random.random([V,T])\n",
        "\n",
        "def softmax(logits):\n",
        "    max_value = np.max(logits, axis=0, keepdims=True)\n",
        "    exp = np.exp(logits - max_value)\n",
        "    exp_sum = np.sum(exp, axis=0, keepdims=True)\n",
        "    dist = exp / exp_sum\n",
        "    return dist\n",
        "  \n",
        "y = softmax(logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YHAZYIwgpZc3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Forward/Backward Dynamic Programming\n",
        "\n",
        "整個 CTC 關鍵就在計算 Dynamic Programming Table, 我們需要計算如下圖的 DP Table:\n",
        "<img src=\"https://i.imgur.com/lOPaABD.png\" height=\"70%\" width=\"70%\">\n"
      ]
    },
    {
      "metadata": {
        "id": "DVIvRsnXiv6l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def forward(y, label):\n",
        "  L = len(label)\n",
        "  V, T = y.shape\n",
        "  alpha = np.zeros([L,T])\n",
        "  # init first column\n",
        "  alpha[0,0] = y[label[0],0]  # TODO\n",
        "  alpha[1,0] = y[label[1],0]  # TODO\n",
        "  # run dp\n",
        "  for t in range(1,T):\n",
        "    for s in range(L):\n",
        "      k = label[s]\n",
        "      y_k_t = y[k,t]\n",
        "      alpha_tmp = alpha[s,t-1]\n",
        "      if s>0:\n",
        "        alpha_tmp += alpha[s-1,t-1]  # TODO\n",
        "      if s>1 and k!=0 and k!=label[s-2]:\n",
        "        alpha_tmp += alpha[s-2,t-1]  # TODO\n",
        "      alpha[s,t] = alpha_tmp*y_k_t\n",
        "  return alpha"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y78iljgTAWlg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def backward(y,label):\n",
        "  L = len(label)\n",
        "  V, T = y.shape\n",
        "  beta = np.zeros([L,T])\n",
        "  # init last column\n",
        "  beta[-1,-1] = y[label[-1],-1]  # TODO\n",
        "  beta[-2,-1] = y[label[-2],-1]  # TODO\n",
        "  # run dp\n",
        "  for t in range(T-2,-1,-1):\n",
        "    for s in range(L):\n",
        "      k = label[s]\n",
        "      y_k_t = y[k,t]\n",
        "      beta_tmp = beta[s,t+1]\n",
        "      if s<L-1:\n",
        "        beta_tmp += beta[s+1,t+1]  # TODO\n",
        "      if s<L-2 and k!=0 and k!=label[s+2]:\n",
        "        beta_tmp += beta[s+2,t+1]  # TODO\n",
        "      beta[s,t] = beta_tmp*y_k_t\n",
        "  return beta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kWg7qgg5EPbt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "ea81766b-c6c3-4b43-c67e-f3a35273e20d"
      },
      "cell_type": "code",
      "source": [
        "# Forward and Backward likelihood should be very close\n",
        "\n",
        "alpha = forward(y,l)\n",
        "likelihood_by_forward =  alpha[-1,-1] + alpha[-2,-1]\n",
        "print('likelihood_by_forward = {}'.format(likelihood_by_forward))\n",
        "\n",
        "beta = backward(y,l)\n",
        "likelihood_by_backword =  beta[0,0] + beta[1,0]\n",
        "print('likelihood_by_backword = {}'.format(likelihood_by_backword))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "likelihood_by_forward = 1.6616835750017394e-05\n",
            "likelihood_by_backword = 1.6616835750017394e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3nwPd5WCFf5A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Gradient w.r.t. Posterior $y$\n",
        "\n",
        "[論文](https://www.cs.toronto.edu/~graves/icml_2006.pdf)的 (14) 如下:\n",
        "\n",
        "$$\n",
        "p(l|x)=\\sum_{s=1}^{|l'|}\\frac{\\alpha_t(s)\\beta_t(s)}{y_{l_s}^t}\n",
        "$$\n",
        "\n",
        "注意微分時 $\\alpha_t(s)$ 與 $\\beta_t(s)$ 是跟 $y_{l_s}^t$ 有關, 不能視為 constant, 利用它們的定義直接展開求微分即可. 譬如我們以 $t=2$ 當例子, 並且假設只有一條 alignment $\\pi$, 就是先忽略 $\\sum_{\\pi\\in B^{-1}(l)}$, 不影響推導\n",
        "\n",
        "$$\n",
        "p(l|x)=\\sum_{s=1}^{|l'|}\\frac{\\alpha_2(s)\\beta_2(s)}{y_{l'_s}^2}\\\\\n",
        "=\\sum_{s=1}^{|l'|}\\frac{y_{l'_s}^0y_{l'_s}^1(y_{l'_s}^2)^2y_{l'_s}^3...y_{l'_s}^T}{y_{l'_s}^2}\\\\\n",
        "=\\sum_{s=1}^{|l'|} {y_{l'_s}^0y_{l'_s}^1(y_{l'_s}^2)y_{l'_s}^3...y_{l'_s}^T}\\\\\n",
        "\\frac{\\partial p(l|x)}{\\partial y_{l'_s}^2}={y_{l'_s}^0y_{l'_s}^1y_{l'_s}^3...y_{l'_s}^T} \\mbox{,  只保留 index }s\\\\\n",
        "=\\frac{\\alpha_2(s)\\beta_2(s)}{(y_{l'_s}^2)^2}\n",
        "$$\n",
        "\n",
        "因此得到 Gradient:\n",
        "$$\n",
        "\\frac{\\partial p(l|x)}{\\partial y_k^t}=\\frac{1}{{y_k^t}^2}\\sum_{s\\in lab(l,k)}\\alpha_t(s)\\beta_t(s)\n",
        "$$\n",
        "\n",
        "解釋一下 $lab(l,k)$, 以我們上面的例子來說, `l = [0, 3, 0, 3, 0, 4, 0]`, `Vocabulary = [0,1,2,3,4]`, 而 $k$ 是指 Vocabulary 的哪一個值\n",
        "\n",
        "- $lab(l,0)=\\{0,2,4,6\\}$\n",
        "- $lab(l,1)=\\{\\}$\n",
        "- $lab(l,2)=\\{\\}$\n",
        "- $lab(l,3)=\\{1,3\\}$\n",
        "- $lab(l,4)=\\{5\\}$"
      ]
    },
    {
      "metadata": {
        "id": "5syn0MgnblM7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "53191cf3-378f-4a37-8176-b6ada28d2821"
      },
      "cell_type": "code",
      "source": [
        "def lab_l_k(l,k):\n",
        "  return [i for i,s in enumerate(l) if s==k]  # TODO\n",
        "print(lab_l_k(l,0))\n",
        "print(lab_l_k(l,1))\n",
        "print(lab_l_k(l,3))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 2, 4, 6]\n",
            "[]\n",
            "[1, 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fRrwGv8AdnG_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "因此我們的 gradient 計算公式如下\n",
        "$$\n",
        "\\frac{\\partial \\ln p(l|x)}{\\partial y_k^t}=\\frac{1}{p(l|x)}\\frac{\\partial p(l|x)}{\\partial y_k^t}\n",
        "$$"
      ]
    },
    {
      "metadata": {
        "id": "h8rxUftKFdZS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gradient(y, label):\n",
        "  L = len(label)\n",
        "  V, T = y.shape\n",
        "  alpha = forward(y,label)\n",
        "  beta = backward(y,label)\n",
        "  grad = np.zeros([V,T])\n",
        "  p = alpha[-1,-1] + alpha[-2,-1]\n",
        "  for t in range(T):\n",
        "    for k in range(V):\n",
        "      for s in lab_l_k(label,k):\n",
        "        grad[k,t] += alpha[s,t]*beta[s,t]  # TODO\n",
        "  grad /= (y*y*p)\n",
        "  return grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DinVl4zLeGMh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "cf7c2576-c113-44ba-e351-04e369ab2059"
      },
      "cell_type": "code",
      "source": [
        "grad = gradient(y, l)\n",
        "print(grad)\n",
        "print(grad.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2.42731589 2.38710772 2.262184   2.13646168 2.50572836 2.39280472\n",
            "  2.22245005 2.18506772 1.79524329 2.79289545 1.94578502 2.51547533]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.        ]\n",
            " [1.96743327 2.61185213 2.49759386 2.01138397 2.21057073 2.0808029\n",
            "  2.18409621 2.01125349 1.47848018 1.05725284 0.33662066 0.        ]\n",
            " [0.         0.         0.         0.01093027 0.06206987 0.18841371\n",
            "  0.54099205 0.80817959 1.71471278 2.7975887  2.6239225  2.10442376]]\n",
            "(5, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Aie-FDvceV4E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "用數值方法確認 gradient 計算, $\\frac{df(x)}{dx}\\simeq \\frac{f(x+\\delta)-f(x-\\delta)}{2\\delta}$, 如果我們只用 $\\frac{df(x)}{dx}\\simeq \\frac{f(x+\\delta)-f(x)}{\\delta}$ 可以嗎? 是可以, 但是精確度較差, 原因可以由泰勒展開式知道:\n",
        "\n",
        "$$\n",
        "f(x+\\delta)=f(x)+f'(x)\\delta+\\frac{1}{2}f''(x)\\delta^2+o(|\\delta|^3)\\\\\n",
        "f(x-\\delta)=f(x)-f'(x)\\delta+\\frac{1}{2}f''(x)\\delta^2+o(|\\delta|^3)\\\\\n",
        "$$\n",
        "\n",
        "比較兩種方式的計算結果:\n",
        "\n",
        "$$\n",
        "\\frac{f(x+\\delta)-f(x-\\delta)}{2\\delta}=f'(x)+o(|\\delta|^2)\\\\\n",
        "\\frac{f(x+\\delta)-f(x)}{\\delta}=f'(x)+\\frac{1}{2}f''(x)\\delta+o(|\\delta|^2)=f'(x)+o(|\\delta|)\n",
        "$$\n",
        "\n",
        "可以發現 $\\frac{df(x)}{dx}\\simeq \\frac{f(x+\\delta)-f(x)}{\\delta}$ 估計的 error 高出一個 order."
      ]
    },
    {
      "metadata": {
        "id": "rgdDwpBeeVQE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "c8b09942-93c3-4274-9e97-6a2cd258ad12"
      },
      "cell_type": "code",
      "source": [
        "def check_grad(y, label, k=-1, t=-1, toleration=1e-3):\n",
        "  grad_1 = gradient(y, label)[k, t]\n",
        "\n",
        "  delta = 1e-10\n",
        "  original = y[k, t]\n",
        "\n",
        "  y[k, t] = original + delta\n",
        "  alpha = forward(y, label)\n",
        "  log_p1 = np.log(alpha[-1,-1] + alpha[-2,-1])\n",
        "\n",
        "  y[k, t] = original - delta\n",
        "  alpha = forward(y, label)\n",
        "  log_p2 = np.log(alpha[-1,-1] + alpha[-2,-1])\n",
        "\n",
        "  y[k, t] = original\n",
        "\n",
        "  grad_2 = (log_p1 - log_p2) / (2 * delta)  # TODO\n",
        "  if np.abs(grad_1 - grad_2) > toleration:\n",
        "    print('[%d, %d]：%.2e' % (k, t, np.abs(grad_1 - grad_2)))\n",
        "\n",
        "for toleration in [1e-5, 1e-6]:\n",
        "  print('%.e' % toleration)\n",
        "  for w in range(y.shape[0]):\n",
        "    for v in range(y.shape[1]):\n",
        "      check_grad(y, l, w, v, toleration)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1e-05\n",
            "1e-06\n",
            "[0, 0]：4.20e-06\n",
            "[0, 1]：5.01e-06\n",
            "[0, 2]：2.44e-06\n",
            "[0, 3]：1.79e-06\n",
            "[0, 5]：1.23e-06\n",
            "[0, 6]：5.60e-06\n",
            "[0, 7]：2.19e-06\n",
            "[0, 8]：5.11e-06\n",
            "[0, 11]：3.05e-06\n",
            "[3, 0]：2.61e-06\n",
            "[3, 1]：5.26e-06\n",
            "[3, 3]：6.24e-06\n",
            "[3, 4]：7.66e-06\n",
            "[3, 5]：5.15e-06\n",
            "[3, 6]：3.30e-06\n",
            "[3, 10]：1.04e-06\n",
            "[4, 3]：3.21e-06\n",
            "[4, 4]：4.92e-06\n",
            "[4, 5]：4.43e-06\n",
            "[4, 6]：6.31e-06\n",
            "[4, 8]：4.40e-06\n",
            "[4, 9]：4.57e-06\n",
            "[4, 10]：3.64e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uNSuvN4ngghn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Gradient w.r.t. Logits $u$\n",
        "\n",
        "<img src=\"https://i.imgur.com/1k0g7N2.png\" height=\"40%\" width=\"40%\">\n",
        "\n",
        "如上圖, 我們已經計算了 $\\frac{\\partial L}{\\partial y_i}$ (這邊假定固定的 $t$, 因此省略不寫), 我們希望計算 $\\frac{\\partial L}{\\partial u_k}$, 由 chain rule 知道如下關係\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial u_k}=\\sum_i^V\\frac{\\partial L}{\\partial y_i}\\frac{\\partial y_i}{\\partial u_k}\n",
        "$$\n",
        "\n",
        "而 softmax 的 gradient 如下:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial y_i}{\\partial u_k}=y_i(\\delta_{ik}-y_k)\n",
        "$$\n",
        "\n",
        "所以結合起來變成:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial u_k}=\\sum_i^V\\frac{\\partial L}{\\partial y_i}\\frac{\\partial y_i}{\\partial u_k}\\\\\n",
        "=\\sum_i^V \\left(\\frac{1}{p(l|x){y_i}^2}\\sum\\alpha\\beta\\right)y_i(\\delta_{ik}-y_k)\\\\\n",
        "=\\sum_i^V \\left(\\frac{\\sum\\alpha\\beta}{p(l|x)y_i}\\right)(\\delta_{ik}-y_k)\\\\\n",
        "=\\sum_i^V \\left(\\frac{\\sum\\alpha\\beta}{p(l|x)y_i}\\right)\\delta_{ik}-y_k\\sum_i^V \\left(\\frac{\\sum\\alpha\\beta}{p(l|x)y_i}\\right)\\\\\n",
        "=\\frac{\\sum\\alpha\\beta}{p(l|x)y_k}-y_k\n",
        "$$ \n",
        "\n",
        "最後一行的第一個 term 是因為只有 $i=k$時 $\\delta_{ik}=1$, 第二個 term 是因為 $\\sum_i^V\\frac{\\sum\\alpha\\beta}{y_i}=p(l|x)$, 請參考論文 $\\frac{\\alpha\\beta}{y_i}$的物理意義\n",
        "\n",
        "總結來說, 將原來 gradient 計算:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial y_i}=\\frac{\\sum\\alpha\\beta}{p(l|x){y_i}^2}\n",
        "$$\n",
        "改成如下計算:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial u_k}=\\frac{\\sum\\alpha\\beta}{p(l|x)y_k}-y_k\n",
        "$$"
      ]
    },
    {
      "metadata": {
        "id": "fZNt3xUJgXXo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "86b62f58-2bdf-4ba8-ec1a-f9c7ad3e077f"
      },
      "cell_type": "code",
      "source": [
        "def gradient_logits_naive(y, label):\n",
        "  '''\n",
        "  gradient by back propagation\n",
        "  '''\n",
        "  y_grad = gradient(y, label)\n",
        "\n",
        "  sum_y_grad = np.sum(y_grad * y, axis=0, keepdims=True)\n",
        "  u_grad = y * (y_grad - sum_y_grad) \n",
        "\n",
        "  return u_grad\n",
        "\n",
        "def gradient_logits(y, label):\n",
        "  L = len(label)\n",
        "  V, T = y.shape\n",
        "  alpha = forward(y,label)\n",
        "  beta = backward(y,label)\n",
        "  grad = np.zeros([V,T])\n",
        "  p = alpha[-1,-1] + alpha[-2,-1]\n",
        "  for t in range(T):\n",
        "    for k in range(V):\n",
        "      for s in lab_l_k(label,k):\n",
        "        grad[k,t] += alpha[s,t]*beta[s,t]  # TODO\n",
        "  grad /= (y*p)\n",
        "  grad -= y  # TODO\n",
        "  return grad\n",
        "\n",
        "grad_l = gradient_logits_naive(logits, l)\n",
        "grad_2 = gradient_logits(logits, l)\n",
        "\n",
        "print(np.sum(np.abs(grad_l - grad_2)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.713826121631847e-15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k7mqs3_3iiZn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tensorflow Gradient w.r.t. Logits $u$\n",
        "\n",
        "使用 tensorflow [`tf.nn.ctc_loss`](https://www.tensorflow.org/api_docs/python/tf/nn/ctc_loss) 來驗證我們的計算\n",
        "\n",
        "tf.nn.ctc_loss(\n",
        "    **labels,\n",
        "    inputs,\n",
        "    sequence_length**,\n",
        "    preprocess_collapse_repeated=False,\n",
        "    ctc_merge_repeated=True,\n",
        "    ignore_longer_outputs_than_inputs=False,\n",
        "    time_major=True\n",
        ")\n",
        "\n",
        "很多參數, 我們就看主要的前三個\n",
        "- `labels`: An int32 SparseTensor. labels.indices[i, :] == [b, t] means labels.values[i] stores the id for (batch b, time t). labels.values[i] must take on values in [0, **num_labels**). See core/ops/ctc_ops.cc for more details.\n",
        "- `inputs`: 3-D float Tensor. If time_major == False, this will be a Tensor shaped: [batch_size, max_time, **num_classes**]. If time_major == True (default), this will be a Tensor shaped: [max_time, batch_size, **num_classes**]. The logits.\n",
        "- `sequence_length`: 1-D int32 vector, size [batch_size]. The sequence lengths.\n",
        "\n",
        "\n",
        "其中有四點要注意:\n",
        "1. `labels` 用的是 num_labels, 而 `inputs` 用的是 num_classes, 它們的關係為: num_classes = num_labels + 1. 所以 num_labels 表示的是 true labels, 而 num_classes 是多了一個 'blank' 的數量\n",
        "2. 'blank' 的 index 不像我們上面一樣用 0 表示, 會是 num_classes-1.\n",
        "3. tf 的 ctc_loss 用的 input 直接就是 logits $u$, 不像我們上面算 gradient 時給的是後驗概率 $y$\n",
        "4. tf 是 loss, 不是 likelihood, 所以算的 gradient 跟我們上面的正負號相反\n"
      ]
    },
    {
      "metadata": {
        "id": "zmjQfYfF4KN3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 1. 建立 inputs"
      ]
    },
    {
      "metadata": {
        "id": "KSFarG8a4J6Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# logits.shape = y.shape = [V,T]\n",
        "perm_idx = list(range(1,V))+[0]\n",
        "inputs = logits[perm_idx,:].T.reshape([T,1,V]) # [max_time, batch_size, num_classes]\n",
        "inputs_tensor = tf.constant(inputs, dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bwEF7QTY4fTo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2. 建立 labels"
      ]
    },
    {
      "metadata": {
        "id": "-ku-Nf3S4lf-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 原來 l = [0, 3, 0, 3, 0, 4, 0], 在這邊要變成 [4, 2, 4, 2, 4, 3, 4], true labels = {0,1,2,3}, blank={4} (blank 不用自己填)\n",
        "labels_tensor = tf.SparseTensor(indices=[[0, 1], [0, 3], [0, 5]], values=[2, 2, 3], dense_shape=[1, 7])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i_iccAlu5dt2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3. 建立 ctc_loss, 和計算 gradient"
      ]
    },
    {
      "metadata": {
        "id": "HKoPZasV5gmt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "bcb78732-3ffd-4e2d-fd9d-33951fc271d9"
      },
      "cell_type": "code",
      "source": [
        "ctc_loss = tf.nn.ctc_loss(labels_tensor, inputs_tensor, [12])\n",
        "print('inputs_tensor={}\\nctc_loss={}'.format(inputs_tensor,ctc_loss))\n",
        "tf_grad = tf.gradients(ys=ctc_loss,xs=inputs_tensor)\n",
        "with tf.Session() as sess:\n",
        "  tf_grad_results = sess.run(tf_grad)\n",
        "print(np.array(tf_grad_results).shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inputs_tensor=Tensor(\"Const:0\", shape=(12, 1, 5), dtype=float32)\n",
            "ctc_loss=Tensor(\"CTCLoss:0\", shape=(1,), dtype=float32)\n",
            "(1, 12, 1, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Hcw1Vd095tmx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4. 比較 tf 算的 gradient"
      ]
    },
    {
      "metadata": {
        "id": "_OHiPJLP5z4_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2edc72c5-f780-482b-e0c2-753e52fcbcd7"
      },
      "cell_type": "code",
      "source": [
        "# tf 是 loss, 我們之前算的是 likelihood, 因此差一個正負號\n",
        "tf_grad_results = -np.array(tf_grad_results).reshape([T,V]).T\n",
        "\n",
        "# tf 的 'blank' 是最大的 index, 將他挪回 index 0, true index 全部加 1\n",
        "perm_idx = [V-1]+list(range(0,V-1))\n",
        "tf_grad_results = tf_grad_results[perm_idx,:]\n",
        "\n",
        "# print(tf_grad_results)\n",
        "# print(np.array(tf_grad_results).shape)\n",
        "\n",
        "# 我們之前計算的 gradient\n",
        "grad = gradient_logits(y, l)\n",
        "# print(grad)\n",
        "# print(grad.shape)\n",
        "\n",
        "# compare\n",
        "diff_with_tf = np.mean(np.abs(tf_grad_results - grad))\n",
        "print(diff_with_tf)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0472204143052545e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "guASq8jE08Xd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Log-scale Forward/Backward/Gradient\n",
        "\n",
        " 採用 Log scale 的前向後向以及 Gradient 計算會穩定很多"
      ]
    },
    {
      "metadata": {
        "id": "djTvHNhTrcMV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ninf = -np.float('inf')\n",
        "\n",
        "def _logsumexp(a, b):\n",
        "    '''\n",
        "    np.log(np.exp(a) + np.exp(b))\n",
        "\n",
        "    '''\n",
        "    \n",
        "    if a < b:\n",
        "        a, b = b, a\n",
        "        \n",
        "    if b == ninf:\n",
        "        return a\n",
        "    else:\n",
        "        return a + np.log(1 + np.exp(b - a)) \n",
        "    \n",
        "def logsumexp(*args):\n",
        "    '''\n",
        "    from scipy.special import logsumexp\n",
        "    logsumexp(args)\n",
        "    '''\n",
        "    res = args[0]\n",
        "    for e in args[1:]:\n",
        "        res = _logsumexp(res, e)\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hT5PT9DG1HgB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a19ed77-8299-4a9c-d1a6-4e22a04f8356"
      },
      "cell_type": "code",
      "source": [
        "def forward_log(log_y, label):\n",
        "  L = len(label)\n",
        "  V, T = log_y.shape\n",
        "  alpha = np.ones([L,T])*ninf\n",
        "  # init first column\n",
        "  alpha[0,0] = log_y[label[0],0]\n",
        "  alpha[1,0] = log_y[label[1],0]\n",
        "  # run dp\n",
        "  for t in range(1,T):\n",
        "    for s in range(L):\n",
        "      ls = label[s]\n",
        "      log_y_ls_t = log_y[ls,t]\n",
        "      alpha_tmp = alpha[s,t-1]\n",
        "      if s>0:\n",
        "        alpha_tmp = logsumexp(alpha_tmp,alpha[s-1,t-1])  # TODO\n",
        "      if s>1 and ls!=0 and ls!=label[s-2]:\n",
        "        alpha_tmp = logsumexp(alpha_tmp,alpha[s-2,t-1])  # TODO\n",
        "      alpha[s,t] = alpha_tmp + log_y_ls_t\n",
        "  return alpha\n",
        "\n",
        "log_alpha = forward_log(np.log(y), l)\n",
        "alpha = forward(y, l)\n",
        "print(np.sum(np.abs(np.exp(log_alpha) - alpha)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6.624198333817349e-17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ivd945tX2GWE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b885b8fc-67be-4bce-b94f-e0bec24c22ec"
      },
      "cell_type": "code",
      "source": [
        "def backward_log(log_y, label):\n",
        "  L = len(label)\n",
        "  V, T = log_y.shape\n",
        "  beta = np.ones([L,T])*ninf\n",
        "  # init last column\n",
        "  beta[-1,-1] = log_y[label[-1],-1]\n",
        "  beta[-2,-1] = log_y[label[-2],-1]\n",
        "  # run dp\n",
        "  for t in range(T-2,-1,-1):\n",
        "    for s in range(L):\n",
        "      ls = label[s]\n",
        "      log_y_ls_t = log_y[ls,t]\n",
        "      beta_tmp = beta[s,t+1]\n",
        "      if s<L-1:\n",
        "        beta_tmp = logsumexp(beta_tmp,beta[s+1,t+1])\n",
        "      if s<L-2 and ls!=0 and ls!=label[s+2]:\n",
        "        beta_tmp = logsumexp(beta_tmp,beta[s+2,t+1])\n",
        "      beta[s,t] = beta_tmp + log_y_ls_t\n",
        "  return beta\n",
        "\n",
        "log_beta = backward_log(np.log(y), l)\n",
        "beta = backward(y, l)\n",
        "print(np.sum(np.abs(np.exp(log_beta) - beta)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7.861201279520589e-17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ou_iq-Vr2oAM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e1f319ad-d8e1-4d06-a83b-866ab3f5e35a"
      },
      "cell_type": "code",
      "source": [
        "def gradient_log(log_y, label):\n",
        "  L = len(label)\n",
        "  V, T = log_y.shape\n",
        "  log_alpha = forward_log(log_y,label)\n",
        "  log_beta = backward_log(log_y,label)\n",
        "  log_grad = np.ones([V,T])*ninf\n",
        "  log_p = logsumexp(log_alpha[-1,-1], log_alpha[-2,-1])\n",
        "  for t in range(T):\n",
        "    for k in range(V):\n",
        "      for s in lab_l_k(label,k):\n",
        "        log_grad[k,t] = logsumexp(log_grad[k,t],log_alpha[s,t]+log_beta[s,t])  # TODO\n",
        "  log_grad -= (2*log_y+log_p)\n",
        "  return log_grad\n",
        "\n",
        "log_grad = gradient_log(np.log(y), l)\n",
        "grad = gradient(y, l)\n",
        "print(np.sum(np.abs(np.exp(log_grad) - grad)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.215619602871868e-14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6oOggns39hxO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Scaling 方法跳過"
      ]
    },
    {
      "metadata": {
        "id": "thdZuTmKu9jZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tensorflow ctc_decoder\n",
        "\n",
        "使用上沒啥好說的, 有一點要注意的是如同上面說的 tf 會將最大的 label 視為 'blank', 所以下面 decode 會看到 label 0 但是沒有 label 4, 另外, 預設 merge_repeated=True 會去掉 blank 和 重複"
      ]
    },
    {
      "metadata": {
        "id": "pXDDZaYMypU_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "tf.nn.ctc_greedy_decoder(\n",
        "    inputs,\n",
        "    sequence_length,\n",
        "    merge_repeated=True\n",
        ")"
      ]
    },
    {
      "metadata": {
        "id": "6mnBUFH5vQXk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "26135a0d-48a6-49d1-86dd-67d02e6b72af"
      },
      "cell_type": "code",
      "source": [
        "decoded, neg_sum_logits = tf.nn.ctc_greedy_decoder(inputs_tensor, [12])\n",
        "with tf.Session() as sess:\n",
        "  decoded_out, neg_sum_logits_out = sess.run([decoded, neg_sum_logits])\n",
        "print(decoded_out)\n",
        "print(neg_sum_logits_out)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[SparseTensorValue(indices=array([[0, 0],\n",
            "       [0, 1],\n",
            "       [0, 2],\n",
            "       [0, 3]]), values=array([0, 1, 3, 0]), dense_shape=array([1, 4]))]\n",
            "[[-9.365427]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oWB5DXp_yRm4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "tf.nn.ctc_beam_search_decoder(\n",
        "    inputs,\n",
        "    sequence_length,\n",
        "    beam_width=100,\n",
        "    top_paths=1,\n",
        "    merge_repeated=True\n",
        ")"
      ]
    },
    {
      "metadata": {
        "id": "8yWBVuD3yWZc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "0ebfc825-aa9d-4371-80ab-2507b7f31ef9"
      },
      "cell_type": "code",
      "source": [
        "decoded_beam, neg_sum_logits_beam = tf.nn.ctc_greedy_decoder(inputs_tensor, [12])\n",
        "with tf.Session() as sess:\n",
        "  decoded_beam_out, neg_sum_logits_beam_out = sess.run([decoded_beam, neg_sum_logits_beam])\n",
        "print(decoded_beam_out)\n",
        "print(neg_sum_logits_beam_out)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[SparseTensorValue(indices=array([[0, 0],\n",
            "       [0, 1],\n",
            "       [0, 2],\n",
            "       [0, 3]]), values=array([0, 1, 3, 0]), dense_shape=array([1, 4]))]\n",
            "[[-9.365427]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RB0ryUsVzirA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "更多 decoding 的算法細節, 請參考 [DingKe](https://github.com/DingKe/ml-tutorial/blob/master/ctc/CTC.ipynb) 的文章"
      ]
    },
    {
      "metadata": {
        "id": "7aqnGyac4cxq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}